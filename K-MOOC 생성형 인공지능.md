# K-MOOC 생성형 인공지능

## 1주차

### ChatGPT

* 초거대 언어모델 `Large Language Model, LLM`
* 트랜스포머 Decoder모델인 생성형 사전 학습 트랜스포머(`Generative Pre-trained Transformer`)를 기반으로 한다.
* GPT 전의 모델은 `BERT(Bidirectional Encoder Representations from Trnsformers)`
* 사용자 query를 기반으로 텍스트 응답을 생성

### GPT Architecture

* GPT는 트랜스포머 디코더 모델 기반
* 트랜스포머 디코더 모델은 `Masked Multi-Head Self-Attention(Masked MSA)`

### ChatGPT는 어떻게 훈련?

* GPT-3.5는 대규모 언어 데이터 세트를 이용하여 사전 학습된다.
* `강화 학습`을 통한 `사용자 피드백(RLHF)`를 통한 `미세 조정(Fine-tuning)`

### ChatGPT의 한계

* 때때로 사실에 근거하지 않는 부정확한 답변 생성
* 불확실한 query가 제공되면 `추측`을 시도

### 생성형 인공지능

* `생성형 인공지능(Generative AI)`는 텍스트, 이미지, 음악 등의 콘텐츠 생성을 중점으로 둔 최근 인공지능(AI)의 한 분야이다.
* 대표적인 플랫폼으로는 ChatGPT와 DALL-E가 있다.

### 생성형 인공지능 출력의 종류

* 이미지 생성
  * `GAN(Generative Adversarial Networks)`
  * 오토 인코더(Autoencoder)를 사용하여 실제 이미지 생성 가능
* 텍스트 생성
  * `순환 신경망(RNN)`이나 `트랜스포머(Transformer)`를 사용하여 몇 가지 유용한 텍스트 생성 가능 
* 비디오 생성
  * 3D CNN, GAN
* 오디오 생성
  * 1D CNN, GAN

### 생성형 인공지능 중요 구성요소

* 생성형 인공지능은 중요한 2가지 부분으로 구성
  * `Encoder`와 `Decoder`
  * 인코더는 입력을 해석하고 `Contextual Information` 생성
  * 디코더는 `Contextual Information`을 기반으로 출력을 생성

### 생성형 AI 네트워크의 다양한 구조

* Generative Adversarial Network(GAN)
  * `판별기(Discriminator)`와 `생성기`로 구성
* Variational Autoencoder(VAE)
  * 표준 인코더-디코더 아키텍처 판별기
* Flow-based Models
  * 디코더는 인코더의 역함수
* Diffusion Models
  * 노이즈 생성 및 제거 수행

### 생성형 AI 문제점

* 데이터셋
  * 대규모 모델을 생성하기 위해선 충분한 수의 고품질 데이터 필요
  * 현재는 `데이터 증강`, `전이 학습`으로 해결
* 보안

### 인지 인공지능

* `인지 인공지능(Cognitive AI)`는 컴퓨터가 인간의 사고와 인지 과정을 시뮬레이션하는 방법을 주로 연구하는 인공지능의 한 분야
* 데이터 마이닝, 머신 러닝, 자연어 처리를 활용해 작업 수행

### 인지 인공지능의 구성요소

* 지식표현
  * 다양한 종류의 지식을 설명 및 정리, 지식 간의 추론 및 사용 지원
* 학습
  * 방대한 양의 데이터에서 패턴 자동 학습, 자체 모델 지속적 최적화
* 자연어 처리
  * 컴퓨터가 자연어로 인간의 의사 소통을 이해하고 처리하도록 지원
* 컴퓨터 비전
  * 방대한 양의 데이터에서 패턴을 자동으로 학습 & 자체 모델 최적화
* 추론 및 의사 결정
  * 기존 정보를 기반으로 추론 & 의사 결정 - 최상의 솔루션 제공

### 인지 AI의 응용

* 지능형 고객서비스, 검색, 번역, 이미지 분석, 자율 주행

### 인지 AI와 생성 AI의 차이점

* 구성 요소는 비슷하지만 목적이 다르다.
* 인지 AI는 인간과 같은 인지 능력을 발휘할 수 있는 시스템 개발
* 생성 AI는 이미지와 텍스트 등의 데이터를 생성하는 시스템을 개발하는 것을 목표

### 언어 생성

* `언어 생성(Language Generation)`은 컴퓨터 프로그램을 사용하여 자연스러운 인간 언어의 규범에 부합하는 텍스트를 생성하는 프로세스
* `자연어 처리(Natural Language Process)`의 중요한 분야
* 언어 생성에 사용되는 모델
  * `마르코프 체인`
  * `순환 신경망(RNN)`
  * `트랜스포머` 등

### 자연어 생성 네트워크

* `자연어 생성 네트워크(Network for Neural language Generative)`는 언어 생성에 대한 `딥러닝 기반 접근 방식`으로, `신경망을 모델링`하여 언어의 법칙과 특징을 학습하고 새로운 텍스트를 생성
* `신경망 생성 모델링`의 주요 장점은 `문맥 정보를 처리`하고 자연스럽고 유창하며 표현력 풍부한 텍스트 생성, 수동으로 설계된 기능이나 규칙이 없음

### 딥러닝 기반 이미지(& 영상?) 생성

* 입력 이미지와 출력 이미지 사이의 매핑 관계를 학습하도록 훈련된 `신경망 모델`을 사용
* 고품질의 사실적 이미지를 생성할 수 있는 `생성 적대적 신경망(Generative Adversarial Network)`, `가변적 자동 인코더(Variational Auto-Encoder)` 등이 있음
* `GAN`
  * 이미지 생성을 담당하는 `생성기(Generator)`와 생성된 이미지의 진위 여부를 판단하는 `판별기(Discriminator)`의 2가지 신경망으로 구성됨
* `VAE`
  * 입력 영상의 잠재 변수(Latent Vector) 분포를 학습한 다음 이 분포를 기반으로 새 이미지를 생성하는 자동 인코더 기반 모델


### 언어 생성 방법

1. 데이터셋 선택
   * 언어 생성을 위한 데이터 집합을 `텍스트 코퍼스(Text Corpus)`라고 한다.
   * 여러 문장으로 구성되며 클래스, 품사 또는 번역과 같은 정보를 포함할 수 있음.
2. 전처리
   * `토큰화`는 텍스트를 다른 입력 요소로 분리하는 프로세스
   * `벡터화`는 토큰을 숫자 표현으로 변환하는 과정
   * `임베딩`은 의미 관계 또는 문맥적 의미를 추가하여 벡터 표현을 더욱 개선
3. 언어모델 선택
   * `순환 신경망(RNN)`
     * 메모리를 저장하고 시간적 종속성을 포착하는 순차적 숨겨진 상태 사용
   * `트랜스포머(Transformers)`
     * 모든 입력 토큰에 문맥 관계를 생성하는 멀티 헤드 셀프 어텐션 사용
4. 모델 훈련
   1. 무작위 가중치로 모델 초기화
   2. 학습 데이터를 모델에 공급, 예측 값 얻음
   3. 오차 계산
   4. 역전파 & 최적화 알고리즘 이용 - 모델 가중치 업데이터 & 손실(오차) 최소화
   5. 전체 데이터 세트에 대해 2~4단계를 여러 번 반복하여 모델의 성능 개선
5. 모델 평가
   * 다음 지표를 사용
     * 난해성 - 모델이 시퀀스에서 다음 단어를 얼마자 잘 예측하는지 측정
     * 이중 언어 평가 연구(BLEU) 점수 - 생성된 텍스트와 하나 이상의 참조 텍스트 간의 유사성을 측정
     * 생략

# 2주차

### 재귀 신경망

* ANN, CNN 내의 요소는 독립적
* RNN은 순차적, 시계열 데이터를 위해 설계됨
* 방향성 비순환 그래프로 표시
* 단방향 RNN 구조는 전방향 레이어만 사용
  * 모델은 과거 정보에만 접근 가능 & 다음 출력 토큰 예측하는 데 유용
* 양방향 RNN은 전/후 방향 레이어 모두 사용
  * 과거 & 미래 정보 모두 액세스 가능
  * 전체 입력 시퀀스 사용할 수 있어야 함.

### RNN의 문제점

* RNN은 장기적 의존성을 가지고 있다.
* 처리 오래 걸리면 - 오래 전의 기억을 할 수 없음
  * gradient가 없어지거나(Vanishing) 폭발적 증가(Exploding)할 수 있다.

### RNN 개선

* LSTM, GRU 이용
* 소실 그라디언트
  * 기억을 저장하기 때문에 이전의 큰 기억은 단순 RNN처럼 바로 지워지지 않음.
* 폭발 그라디언트
  * 그라디언트 클리핑 - 임계값 미만 or 초과시 한계값으로 설정

### 장-단기 메모리(LSTM)

* 3가지 유형의 게이트 사용
  * Forget gate - 보관하거나 버릴 정보 제어
  * Input gate - 셀 상태에 대한 입력 정보 계산
  * Output gate - 숨겨진 상태 결정
* 중요한 부분은 LSTM 네트워크의 메모리 역할을 하는 셀 상태(Cell State)

### 게이트형 재귀 장치(Gated Recuurent Unit)

* 2가지 유형의 게이트 사용 - LSTM을 간소화
  * LSTM의 cell state가 없고 hidden state가 대신
  * Forget & Input gate가 Update gate로 결합
  * Output gate와 비슷한 역할하는 Reset gate 추가

### RNN의 인코더와 디코더

